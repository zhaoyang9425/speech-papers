# Summary of speech papers

## Speech Recognition

### New Models
Kubo Y, Bacchiani M. Joint Phoneme-Grapheme Model for End-To-End Speech Recognition[C]//ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2020: 6119-6123.
[[paper](https://storage.googleapis.com/pub-tools-public-publication-data/pdf/45885a679869bf40528dcf46cfd795a3003eeb5e.pdf)][[videa](https://2020.ieeeicassp-virtual.org/presentation/lecture/joint-phoneme-grapheme-model-end-end-speech-recognition)]

Kriman S, Beliaev S, Ginsburg B, et al. Quartznet: Deep automatic speech recognition with 1d time-channel separable convolutions[C]//ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2020: 6124-6128.
[[paper](https://arxiv.org/pdf/1910.10261)][[video](https://2020.ieeeicassp-virtual.org/presentation/lecture/quartznet-deep-automatic-speech-recognition-1d-time-channel-separable)]

Tripathi A, Lu H, Sak H. End-To-End Multi-Talker Overlapping Speech Recognition[C]//ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2020: 6129-6133.
[[paper](https://www.merl.com/publications/docs/TR2018-001.pdf)][[video](https://2020.ieeeicassp-virtual.org/presentation/lecture/end-end-multi-talker-overlapping-speech-recognition)]

Chang X, Zhang W, Qian Y, et al. End-To-End Multi-Speaker Speech Recognition With Transformer[C]//ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2020: 6134-6138.
[[paper](https://arxiv.org/pdf/2002.03921)][[video](https://www.youtube.com/watch?v=Mo2RRNgfU4g)]

Variani E, Rybach D, Allauzen C, et al. Hybrid autoregressive transducer (HAT)[C]//ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2020: 6139-6143.
[[paper](https://arxiv.org/pdf/2003.07705)][[video](https://2020.ieeeicassp-virtual.org/presentation/lecture/hybrid-autoregressive-transducer-hat)]

Winata G I, Cahyawijaya S, Lin Z, et al. Lightweight and Efficient End-to-End Speech Recognition Using Low-Rank Transformer[C]//ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2020: 6144-6148.
[[paper](https://arxiv.org/pdf/1910.13923)][[video](https://2020.ieeeicassp-virtual.org/presentation/lecture/lightweight-and-efficient-end-end-speech-recognition-using-low-rank)]

J. Li, R. Zhao, H. Hu, and Y. Gong, “Improving RNN transducer modeling for end-to-end speech recognition,” arXiv preprint arXiv:abs/1909.12415, 2019. 
[[paper](https://arxiv.org/pdf/1909.12415)]

Moritz N, Hori T, Le Roux J. Streaming End-to-End Speech Recognition with Joint CTC-Attention Based Models[C]//Proc. IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU). 2019. 
[[paper](https://www.merl.com/publications/docs/TR2019-159.pdf)]

Pratap V, Hannun A, Xu Q, et al. wav2letter++: The fastest open-source speech recognition system[J]. arXiv preprint arXiv:1812.07625, 2018.
[[paper](https://arxiv.org/pdf/1812.07625)][[code](https://github.com/facebookresearch/wav2letter)]

Chan W, Jaitly N, Le Q V, et al. Listen, attend and spell[J]. arXiv preprint arXiv:1508.01211, 2015. 
[[paper](https://arxiv.org/abs/1508.01211)]

Graves A. Sequence transduction with recurrent neural networks[J]. arXiv preprint arXiv:1211.3711, 2012. 
[[paper](https://arxiv.org/abs/1211.3711)]

#### Speech Transformer
Dong L, Xu S, Xu B. Speech-transformer: a no-recurrence sequence-to-sequence model for speech recognition[C]//2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2018: 5884-5888.
[[paper](http://150.162.46.34:8080/icassp2018/ICASSP18_USB/pdfs/0005884.pdf)][[PyTorch](https://github.com/kaituoxu/Speech-Transformer)][[PyTorch](https://github.com/ZhengkunTian/Speech-Tranformer-Pytorch)][[TensorFlow](https://github.com/xingchensong/Speech-Transformer-tf2.0)]

Karita S, Chen N, Hayashi T, et al. A comparative study on transformer vs rnn in speech applications[J]. arXiv preprint arXiv:1909.06317, 2019.
[[paper](https://arxiv.org/pdf/1909.06317)][[PyTorch](https://github.com/espnet/espnet)]

Zhou S, Dong L, Xu S, et al. Syllable-based sequence-to-sequence speech recognition with the transformer in mandarin chinese[J]. arXiv preprint arXiv:1804.10752, 2018.
[[paper](https://arxiv.org/pdf/1804.10752)][[PyTorch](https://github.com/gentaiscool/end2end-asr-pytorch)]

Zhou S, Xu S, Xu B. Multilingual end-to-end speech recognition with a single transformer on low-resource languages[J]. arXiv preprint arXiv:1806.05059, 2018.
[[paper](https://arxiv.org/pdf/1806.05059)]

### Streaming
Moritz N, Hori T, Roux J L. Streaming automatic speech recognition with the transformer model[J]. arXiv preprint arXiv:2001.02674, 2020. 
[[paper](https://arxiv.org/abs/2001.02674)]

### Self-Supervised or Pretraining
Rivière M, Joulin A, Mazaré P E, et al. Unsupervised pretraining transfers well across languages[C]//ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2020: 7414-7418.
[[paper](https://arxiv.org/pdf/2002.02848.pdf)][[code](https://github.com/facebookresearch/CPC_audio)]

Chen Y C, Yang Z, Yeh C F, et al. Aipnet: Generative Adversarial Pre-Training of Accent-Invariant Networks for End-To-End Speech Recognition[C]//ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2020: 6979-6983.
[[paper](https://arxiv.org/pdf/1911.11935.pdf)]

Chung Y A, Glass J. Generative pre-training for speech with autoregressive predictive coding[C]//ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2020: 3497-3501.
[[paper](https://arxiv.org/pdf/1910.12607.pdf)]

Schneider S, Baevski A, Collobert R, et al. wav2vec: Unsupervised pre-training for speech recognition[J]. arXiv preprint arXiv:1904.05862, 2019. 
[[paper](https://arxiv.org/pdf/1904.05862)][[PyTorch](https://github.com/pytorch/fairseq/blob/master/fairseq/models/wav2vec.py)]

Baevski A, Auli M, Mohamed A. Effectiveness of self-supervised pre-training for speech recognition[J]. arXiv preprint arXiv:1911.03912, 2019.
[[paper](https://arxiv.org/pdf/1911.03912.pdf)]

Oord A, Li Y, Vinyals O. Representation learning with contrastive predictive coding[J]. arXiv preprint arXiv:1807.03748, 2018.
[[paper](https://arxiv.org/pdf/1807.03748.pdf)][[code](https://github.com/davidtellez/contrastive-predictive-coding)]

Hyvarinen A, Morioka H. Unsupervised feature extraction by time-contrastive learning and nonlinear ICA[C]//Advances in Neural Information Processing Systems. 2016: 3765-3773.
[[paper](https://papers.nips.cc/paper/6395-unsupervised-feature-extraction-by-time-contrastive-learning-and-nonlinear-ica.pdf)][[Tensorflow](https://github.com/hirosm/TCL)]



### Attention
Chiu C C, Raffel C. Monotonic chunkwise attention[J]. arXiv preprint arXiv:1712.05382, 2017. 
[[paper](https://arxiv.org/pdf/1712.05382)]

Moritz N, Hori T, Le Roux J. Triggered attention for end-to-end speech recognition[C]//ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2019: 5666-5670. 
[[paper](http://www.merl.com/publications/docs/TR2019-015.pdf)]

Watanabe S, Hori T, Kim S, et al. Hybrid CTC/attention architecture for end-to-end speech recognition[J]. IEEE Journal of Selected Topics in Signal Processing, 2017, 11(8): 1240-1253. 
[[paper](http://www.merl.com/publications/docs/TR2017-190.pdf)]

Hori T, Watanabe S, Zhang Y, et al. Advances in joint CTC-attention based end-to-end speech recognition with a deep CNN encoder and RNN-LM[J]. arXiv preprint arXiv:1706.02737, 2017. 
[[paper](https://arxiv.org/pdf/1706.02737)]

J. Li, R. Zhao, H. Hu, and Y. Gong, “Improving RNN transducer modeling for end-to-end speech recognition,” arXiv preprint arXiv:abs/1909.12415, 2019. 
[[paper](https://arxiv.org/pdf/1909.12415)]

[[语音中的attention机制的发展与应用](https://github.com/aaaceo890/Attention)]

### Survey
Karita S, Chen N, Hayashi T, et al. A comparative study on transformer vs rnn in speech applications[J]. arXiv preprint arXiv:1909.06317, 2019. [[paper](https://arxiv.org/pdf/1909.06317)]

Prabhavalkar R, Rao K, Sainath T N, et al. A Comparison of Sequence-to-Sequence Models for Speech Recognition[C]//Interspeech. 2017: 939-943. [[paper](https://pdfs.semanticscholar.org/6cc6/8e8adf34b580f3f37d1bd267ee701974edde.pdf)]

### Audio-Visual Speech Recognition
Petridis S, Stafylakis T, Ma P, et al. Audio-visual speech recognition with a hybrid ctc/attention architecture[C]//2018 IEEE Spoken Language Technology Workshop (SLT). IEEE, 2018: 513-520. 
[[paper](https://arxiv.org/pdf/1810.00108)]

## Speech Augmentation
Park D S, Chan W, Zhang Y, et al. Specaugment: A simple data augmentation method for automatic speech recognition[J]. arXiv preprint arXiv:1904.08779, 2019. 
[[paper](https://arxiv.org/pdf/1904.08779.pdf?source=post_page---------------------------)]

Wang C, Wu Y, Du Y, et al. Semantic mask for transformer based end-to-end speech recognition[J]. arXiv preprint arXiv:1912.03010, 2019.
[[paper](https://arxiv.org/pdf/1912.03010)][[code](https://github.com/MarkWuNLP/SemanticMask)][[video](https://www.youtube.com/watch?v=v98BSaVt-Tw)]

## Voice Conversion
Kameoka H, Kaneko T, Tanaka K, et al. StarGAN-VC: Non-parallel many-to-many voice conversion using star generative adversarial networks[C]//2018 IEEE Spoken Language Technology Workshop (SLT). IEEE, 2018: 266-273.
[[paper](https://arxiv.org/pdf/1806.02169)][[PyTorch](https://github.com/liusongxiang/StarGAN-Voice-Conversion)]

Serrà J, Pascual S, Perales C S. Blow: a single-scale hyperconditioned flow for non-parallel raw-audio voice conversion[C]//Advances in Neural Information Processing Systems. 2019: 6790-6800.
[[paper](http://papers.nips.cc/paper/8904-blow-a-single-scale-hyperconditioned-flow-for-non-parallel-raw-audio-voice-conversion.pdf)][[PyTorch](https://github.com/joansj/blow)]

Qian K, Zhang Y, Chang S, et al. Zero-shot voice style transfer with only autoencoder loss[J]. arXiv preprint arXiv:1905.05879, 2019.
[[paper](https://arxiv.org/pdf/1905.05879)][[Pytorch](https://github.com/liusongxiang/StarGAN-Voice-Conversion)]

Chou J, Yeh C, Lee H, et al. Multi-target voice conversion without parallel data by adversarially learning disentangled audio representations[J]. arXiv preprint arXiv:1804.02812, 2018.
[[paper](https://arxiv.org/pdf/1804.02812)][[PyTorch](https://github.com/jjery2243542/voice_conversion)]

