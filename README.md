# Summary of speech papers

## Speech Recognition

### Architecture
Moritz N, Hori T, Roux J L. Streaming automatic speech recognition with the transformer model[J]. arXiv preprint arXiv:2001.02674, 2020. [[paper](https://arxiv.org/abs/2001.02674)]

Karita S, Chen N, Hayashi T, et al. A comparative study on transformer vs rnn in speech applications[J]. arXiv preprint arXiv:1909.06317, 2019. [[paper](https://arxiv.org/pdf/1909.06317)]

Moritz N, Hori T, Le Roux J. Streaming End-to-End Speech Recognition with Joint CTC-Attention Based Models[C]//Proc. IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU). 2019. [[paper](https://www.merl.com/publications/docs/TR2019-159.pdf)]

Schneider S, Baevski A, Collobert R, et al. wav2vec: Unsupervised pre-training for speech recognition[J]. arXiv preprint arXiv:1904.05862, 2019. 
[[paper](https://arxiv.org/pdf/1904.05862)][[PyTorch](https://github.com/pytorch/fairseq/blob/master/fairseq/models/wav2vec.py)]

Pratap V, Hannun A, Xu Q, et al. wav2letter++: The fastest open-source speech recognition system[J]. arXiv preprint arXiv:1812.07625, 2018.
[[paper](https://arxiv.org/pdf/1812.07625)][[code](https://github.com/facebookresearch/wav2letter)]

Chan W, Jaitly N, Le Q V, et al. Listen, attend and spell[J]. arXiv preprint arXiv:1508.01211, 2015. [[paper](https://arxiv.org/abs/1508.01211)]

Graves A. Sequence transduction with recurrent neural networks[J]. arXiv preprint arXiv:1211.3711, 2012. [[paper](https://arxiv.org/abs/1211.3711)]

J. Li, R. Zhao, H. Hu, and Y. Gong, “Improving RNN transducer modeling for end-to-end speech recognition,” arXiv preprint arXiv:abs/1909.12415, 2019. [[paper](https://arxiv.org/pdf/1909.12415)]

Prabhavalkar R, Rao K, Sainath T N, et al. A Comparison of Sequence-to-Sequence Models for Speech Recognition[C]//Interspeech. 2017: 939-943. [[paper](https://pdfs.semanticscholar.org/6cc6/8e8adf34b580f3f37d1bd267ee701974edde.pdf)]

Chiu C C, Raffel C. Monotonic chunkwise attention[J]. arXiv preprint arXiv:1712.05382, 2017. [[paper](https://arxiv.org/pdf/1712.05382)]

Moritz N, Hori T, Le Roux J. Triggered attention for end-to-end speech recognition[C]//ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2019: 5666-5670. [[paper](http://www.merl.com/publications/docs/TR2019-015.pdf)]

Watanabe S, Hori T, Kim S, et al. Hybrid CTC/attention architecture for end-to-end speech recognition[J]. IEEE Journal of Selected Topics in Signal Processing, 2017, 11(8): 1240-1253. [[paper](http://www.merl.com/publications/docs/TR2017-190.pdf)]

Dong L, Xu S, Xu B. Speech-transformer: a no-recurrence sequence-to-sequence model for speech recognition[C]//2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2018: 5884-5888. [[paper](http://150.162.46.34:8080/icassp2018/ICASSP18_USB/pdfs/0005884.pdf)]

#### Speech Transformer
Dong L, Xu S, Xu B. Speech-transformer: a no-recurrence sequence-to-sequence model for speech recognition[C]//2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2018: 5884-5888.
[[paper](http://150.162.46.34:8080/icassp2018/ICASSP18_USB/pdfs/0005884.pdf)][[PyTorch](https://github.com/kaituoxu/Speech-Transformer)][[PyTorch](https://github.com/ZhengkunTian/Speech-Tranformer-Pytorch)][[TensorFlow](https://github.com/xingchensong/Speech-Transformer-tf2.0)]

Karita S, Chen N, Hayashi T, et al. A comparative study on transformer vs rnn in speech applications[J]. arXiv preprint arXiv:1909.06317, 2019.
[[paper](https://arxiv.org/pdf/1909.06317)][[PyTorch](https://github.com/espnet/espnet)]

Zhou S, Dong L, Xu S, et al. Syllable-based sequence-to-sequence speech recognition with the transformer in mandarin chinese[J]. arXiv preprint arXiv:1804.10752, 2018.
[[paper](https://arxiv.org/pdf/1804.10752)][[PyTorch](https://github.com/gentaiscool/end2end-asr-pytorch)]

Zhou S, Xu S, Xu B. Multilingual end-to-end speech recognition with a single transformer on low-resource languages[J]. arXiv preprint arXiv:1806.05059, 2018.
[[paper](https://arxiv.org/pdf/1806.05059)]

### Method

Hori T, Watanabe S, Zhang Y, et al. Advances in joint CTC-attention based end-to-end speech recognition with a deep CNN encoder and RNN-LM[J]. arXiv preprint arXiv:1706.02737, 2017. [[paper](https://arxiv.org/pdf/1706.02737)]

Kim S, Hori T, Watanabe S. Joint CTC-attention based end-to-end speech recognition using multi-task learning[C]//2017 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, 2017: 4835-4839. 
[[paper](https://arxiv.org/pdf/1609.06773)][[PyTorch](https://github.com/Alexander-H-Liu/End-to-end-ASR-Pytorch)]

[[语音中的attention机制的发展与应用](https://github.com/aaaceo890/Attention)]

### Audio-Visual Speech Recognition

Petridis S, Stafylakis T, Ma P, et al. Audio-visual speech recognition with a hybrid ctc/attention architecture[C]//2018 IEEE Spoken Language Technology Workshop (SLT). IEEE, 2018: 513-520. 
[[paper](https://arxiv.org/pdf/1810.00108)]

## Speech Augmentation

Park D S, Chan W, Zhang Y, et al. Specaugment: A simple data augmentation method for automatic speech recognition[J]. arXiv preprint arXiv:1904.08779, 2019. [[paper](https://arxiv.org/pdf/1904.08779.pdf?source=post_page---------------------------)]

## Voice Conversion

Kameoka H, Kaneko T, Tanaka K, et al. StarGAN-VC: Non-parallel many-to-many voice conversion using star generative adversarial networks[C]//2018 IEEE Spoken Language Technology Workshop (SLT). IEEE, 2018: 266-273.
[[paper](https://arxiv.org/pdf/1806.02169)][[PyTorch](https://github.com/liusongxiang/StarGAN-Voice-Conversion)]

Serrà J, Pascual S, Perales C S. Blow: a single-scale hyperconditioned flow for non-parallel raw-audio voice conversion[C]//Advances in Neural Information Processing Systems. 2019: 6790-6800.
[[paper](http://papers.nips.cc/paper/8904-blow-a-single-scale-hyperconditioned-flow-for-non-parallel-raw-audio-voice-conversion.pdf)][[PyTorch](https://github.com/joansj/blow)]

Qian K, Zhang Y, Chang S, et al. Zero-shot voice style transfer with only autoencoder loss[J]. arXiv preprint arXiv:1905.05879, 2019.
[[paper](https://arxiv.org/pdf/1905.05879)][[Pytorch](https://github.com/liusongxiang/StarGAN-Voice-Conversion)]

Chou J, Yeh C, Lee H, et al. Multi-target voice conversion without parallel data by adversarially learning disentangled audio representations[J]. arXiv preprint arXiv:1804.02812, 2018.
[[paper](https://arxiv.org/pdf/1804.02812)][[PyTorch](https://github.com/jjery2243542/voice_conversion)]

